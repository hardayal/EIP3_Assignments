{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HARDAYAL_SINGH_BATCH_5_ASSIGNMENT_3C.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "8edSiHPi2N5l",
        "colab_type": "code",
        "outputId": "13c460f9-edd2-4638-b173-6a9e8a145ff6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# https://keras.io/\n",
        "!pip install -q keras\n",
        "import keras\n",
        "\n",
        "import keras\n",
        "from keras.datasets import cifar10\n",
        "from keras.models import Model, Sequential\n",
        "from keras.layers import Dense, Dropout, Flatten, Input, AveragePooling2D, merge, Activation\n",
        "from keras.layers import Conv2D, MaxPooling2D, BatchNormalization\n",
        "from keras.layers import Concatenate\n",
        "from keras.optimizers import Adam\n",
        "from keras.layers.advanced_activations import LeakyReLU\n",
        "from keras.layers import Reshape, Activation, Conv2D, Input, MaxPooling2D, BatchNormalization, Flatten, Dense, Lambda\n",
        "from keras.layers.merge import concatenate\n",
        "\n",
        "# this part will prevent tensorflow to allocate all the avaliable GPU Memory\n",
        "# backend\n",
        "import tensorflow as tf\n",
        "from keras import backend as k\n",
        "\n",
        "# Don't pre-allocate memory; allocate as-needed\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "# Create a session with the above options specified.\n",
        "k.tensorflow_backend.set_session(tf.Session(config=config))\n",
        "\n",
        "# Hyperparameters\n",
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 50\n",
        "l = 10\n",
        "num_filter = 20"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "-hkvQwOH2cCO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "febc75c7-24b5-400e-f5c9-33b9844d24a1"
      },
      "cell_type": "code",
      "source": [
        "# Load CIFAR10 Data\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "img_height, img_width, channel = x_train.shape[1],x_train.shape[2],x_train.shape[3]\n",
        "\n",
        "# convert to one hot encoing \n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 89s 1us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ztoDypc63gEs",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def space_to_depth_x2(x):\n",
        "    return tf.space_to_depth(x, block_size=8)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ahUZLaorfiQH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def create_block(x):\n",
        "  layer = Conv2D(32, (3,3), strides=(1,1), padding='same', use_bias=False)(x)\n",
        "  layer = BatchNormalization()(layer)\n",
        "  layer = LeakyReLU(alpha=0.1)(layer)\n",
        "\n",
        "  layer = Conv2D(64, (3,3), strides=(1,1), padding='same', use_bias=False)(layer)\n",
        "  layer = BatchNormalization()(layer)\n",
        "  layer = LeakyReLU(alpha=0.1)(layer)\n",
        "  \n",
        "  layer = Conv2D(128, (3,3), strides=(1,1), padding='same', use_bias=False)(layer)\n",
        "  layer = BatchNormalization()(layer)\n",
        "  layer = LeakyReLU(alpha=0.1)(layer)\n",
        "  \n",
        "  layer = Conv2D(256, (3,3), strides=(1,1), padding='same', use_bias=False)(layer)\n",
        "  layer = BatchNormalization()(layer)\n",
        "  layer = LeakyReLU(alpha=0.1)(layer)\n",
        "  \n",
        "  layer = MaxPooling2D(pool_size=(2, 2))(layer)\n",
        "  return layer  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SNnGm8Tv2fR1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "c442dd7e-fd3f-46dd-8c24-91ec11449cd9"
      },
      "cell_type": "code",
      "source": [
        "input = Input(shape=(img_height, img_width, channel,))\n",
        "\n",
        "block1 = create_block(input)\n",
        "skip_connection = block1\n",
        "block2 = create_block(block1)\n",
        "block3 = create_block(block2)\n",
        "block4 = create_block(block3)\n",
        "\n",
        "\n",
        "skip_connection = Conv2D(64, (1,1), strides=(1,1), padding='same', name='conv_21', use_bias=False)(skip_connection)\n",
        "skip_connection = BatchNormalization(name='norm_21')(skip_connection)\n",
        "skip_connection = LeakyReLU(alpha=0.1)(skip_connection)\n",
        "skip_connection = Lambda(space_to_depth_x2)(skip_connection)\n",
        "\n",
        "x = concatenate([skip_connection, block4])\n",
        "\n",
        "flatten = Flatten()(x)\n",
        "\n",
        "output = Dense(num_classes, activation='softmax')(flatten)\n",
        "\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jeh0VAxy26NV",
        "colab_type": "code",
        "outputId": "efc6814a-f28d-4b97-c8e1-7297d647443e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2193
        }
      },
      "cell_type": "code",
      "source": [
        "model = Model(inputs=[input], outputs=[output])\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input_1 (InputLayer)            (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_1 (Conv2D)               (None, 32, 32, 32)   864         input_1[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_1 (BatchNor (None, 32, 32, 32)   128         conv2d_1[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 32)   0           batch_normalization_1[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_2 (Conv2D)               (None, 32, 32, 64)   18432       leaky_re_lu_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_2 (BatchNor (None, 32, 32, 64)   256         conv2d_2[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_2 (LeakyReLU)       (None, 32, 32, 64)   0           batch_normalization_2[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_3 (Conv2D)               (None, 32, 32, 128)  73728       leaky_re_lu_2[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_3 (BatchNor (None, 32, 32, 128)  512         conv2d_3[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_3 (LeakyReLU)       (None, 32, 32, 128)  0           batch_normalization_3[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_4 (Conv2D)               (None, 32, 32, 256)  294912      leaky_re_lu_3[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_4 (BatchNor (None, 32, 32, 256)  1024        conv2d_4[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_4 (LeakyReLU)       (None, 32, 32, 256)  0           batch_normalization_4[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_1 (MaxPooling2D)  (None, 16, 16, 256)  0           leaky_re_lu_4[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_5 (Conv2D)               (None, 16, 16, 32)   73728       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_5 (BatchNor (None, 16, 16, 32)   128         conv2d_5[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_5 (LeakyReLU)       (None, 16, 16, 32)   0           batch_normalization_5[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_6 (Conv2D)               (None, 16, 16, 64)   18432       leaky_re_lu_5[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_6 (BatchNor (None, 16, 16, 64)   256         conv2d_6[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_6 (LeakyReLU)       (None, 16, 16, 64)   0           batch_normalization_6[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_7 (Conv2D)               (None, 16, 16, 128)  73728       leaky_re_lu_6[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_7 (BatchNor (None, 16, 16, 128)  512         conv2d_7[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 128)  0           batch_normalization_7[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_8 (Conv2D)               (None, 16, 16, 256)  294912      leaky_re_lu_7[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_8 (BatchNor (None, 16, 16, 256)  1024        conv2d_8[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 256)  0           batch_normalization_8[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 256)    0           leaky_re_lu_8[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_9 (Conv2D)               (None, 8, 8, 32)     73728       max_pooling2d_2[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_9 (BatchNor (None, 8, 8, 32)     128         conv2d_9[0][0]                   \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_9 (LeakyReLU)       (None, 8, 8, 32)     0           batch_normalization_9[0][0]      \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_10 (Conv2D)              (None, 8, 8, 64)     18432       leaky_re_lu_9[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_10 (BatchNo (None, 8, 8, 64)     256         conv2d_10[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_10 (LeakyReLU)      (None, 8, 8, 64)     0           batch_normalization_10[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_11 (Conv2D)              (None, 8, 8, 128)    73728       leaky_re_lu_10[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_11 (BatchNo (None, 8, 8, 128)    512         conv2d_11[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_11 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_11[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_12 (Conv2D)              (None, 8, 8, 256)    294912      leaky_re_lu_11[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_12 (BatchNo (None, 8, 8, 256)    1024        conv2d_12[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_12 (LeakyReLU)      (None, 8, 8, 256)    0           batch_normalization_12[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_3 (MaxPooling2D)  (None, 4, 4, 256)    0           leaky_re_lu_12[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_13 (Conv2D)              (None, 4, 4, 32)     73728       max_pooling2d_3[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_13 (BatchNo (None, 4, 4, 32)     128         conv2d_13[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_13 (LeakyReLU)      (None, 4, 4, 32)     0           batch_normalization_13[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_14 (Conv2D)              (None, 4, 4, 64)     18432       leaky_re_lu_13[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_14 (BatchNo (None, 4, 4, 64)     256         conv2d_14[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_14 (LeakyReLU)      (None, 4, 4, 64)     0           batch_normalization_14[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_15 (Conv2D)              (None, 4, 4, 128)    73728       leaky_re_lu_14[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_15 (BatchNo (None, 4, 4, 128)    512         conv2d_15[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_15 (LeakyReLU)      (None, 4, 4, 128)    0           batch_normalization_15[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "conv_21 (Conv2D)                (None, 16, 16, 64)   16384       max_pooling2d_1[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "conv2d_16 (Conv2D)              (None, 4, 4, 256)    294912      leaky_re_lu_15[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "norm_21 (BatchNormalization)    (None, 16, 16, 64)   256         conv_21[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "batch_normalization_16 (BatchNo (None, 4, 4, 256)    1024        conv2d_16[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_17 (LeakyReLU)      (None, 16, 16, 64)   0           norm_21[0][0]                    \n",
            "__________________________________________________________________________________________________\n",
            "leaky_re_lu_16 (LeakyReLU)      (None, 4, 4, 256)    0           batch_normalization_16[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "lambda_1 (Lambda)               (None, 2, 2, 4096)   0           leaky_re_lu_17[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "max_pooling2d_4 (MaxPooling2D)  (None, 2, 2, 256)    0           leaky_re_lu_16[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "concatenate_1 (Concatenate)     (None, 2, 2, 4352)   0           lambda_1[0][0]                   \n",
            "                                                                 max_pooling2d_4[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 17408)        0           concatenate_1[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 10)           174090      flatten_1[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 1,968,746\n",
            "Trainable params: 1,964,778\n",
            "Non-trainable params: 3,968\n",
            "__________________________________________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "apCwOjvZ4Kts",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# determine Loss function and Optimizer\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tLaFy2AO4TLl",
        "colab_type": "code",
        "outputId": "9fb19b60-3585-4f73-9254-75e3ee3cc625",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1822
        }
      },
      "cell_type": "code",
      "source": [
        "model.fit(x_train, y_train,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(x_test, y_test))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "Train on 50000 samples, validate on 10000 samples\n",
            "Epoch 1/50\n",
            "50000/50000 [==============================] - 112s 2ms/step - loss: 1.4700 - acc: 0.5247 - val_loss: 1.2006 - val_acc: 0.6036\n",
            "Epoch 2/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.9120 - acc: 0.6863 - val_loss: 1.0477 - val_acc: 0.6610\n",
            "Epoch 3/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.7471 - acc: 0.7424 - val_loss: 1.1411 - val_acc: 0.6426\n",
            "Epoch 4/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.6246 - acc: 0.7850 - val_loss: 1.0888 - val_acc: 0.6534\n",
            "Epoch 5/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.5179 - acc: 0.8191 - val_loss: 1.0061 - val_acc: 0.6865\n",
            "Epoch 6/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.4208 - acc: 0.8535 - val_loss: 0.9801 - val_acc: 0.6942\n",
            "Epoch 7/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.3321 - acc: 0.8836 - val_loss: 1.2473 - val_acc: 0.6809\n",
            "Epoch 8/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.2610 - acc: 0.9103 - val_loss: 1.6945 - val_acc: 0.6322\n",
            "Epoch 9/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.2022 - acc: 0.9314 - val_loss: 1.2200 - val_acc: 0.6832\n",
            "Epoch 10/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.1580 - acc: 0.9460 - val_loss: 1.5032 - val_acc: 0.6747\n",
            "Epoch 11/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.1236 - acc: 0.9584 - val_loss: 1.5480 - val_acc: 0.6636\n",
            "Epoch 12/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.1049 - acc: 0.9643 - val_loss: 2.6040 - val_acc: 0.5843\n",
            "Epoch 13/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.1017 - acc: 0.9655 - val_loss: 1.4253 - val_acc: 0.7003\n",
            "Epoch 14/50\n",
            "50000/50000 [==============================] - 103s 2ms/step - loss: 0.0813 - acc: 0.9719 - val_loss: 2.1997 - val_acc: 0.6202\n",
            "Epoch 15/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0755 - acc: 0.9740 - val_loss: 1.4861 - val_acc: 0.7155\n",
            "Epoch 16/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0889 - acc: 0.9684 - val_loss: 1.5255 - val_acc: 0.7129\n",
            "Epoch 17/50\n",
            "50000/50000 [==============================] - 103s 2ms/step - loss: 0.0693 - acc: 0.9756 - val_loss: 1.5858 - val_acc: 0.7186\n",
            "Epoch 18/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0582 - acc: 0.9801 - val_loss: 1.7235 - val_acc: 0.6878\n",
            "Epoch 19/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0577 - acc: 0.9803 - val_loss: 1.6136 - val_acc: 0.7139\n",
            "Epoch 20/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0637 - acc: 0.9773 - val_loss: 1.8808 - val_acc: 0.6893\n",
            "Epoch 21/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0621 - acc: 0.9782 - val_loss: 1.6487 - val_acc: 0.7066\n",
            "Epoch 22/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0407 - acc: 0.9866 - val_loss: 1.6957 - val_acc: 0.7127\n",
            "Epoch 23/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0324 - acc: 0.9892 - val_loss: 1.7787 - val_acc: 0.7165\n",
            "Epoch 24/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0480 - acc: 0.9834 - val_loss: 1.9693 - val_acc: 0.7025\n",
            "Epoch 25/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0666 - acc: 0.9761 - val_loss: 1.7845 - val_acc: 0.7076\n",
            "Epoch 26/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0442 - acc: 0.9845 - val_loss: 1.9290 - val_acc: 0.7098\n",
            "Epoch 27/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0342 - acc: 0.9883 - val_loss: 1.7792 - val_acc: 0.7168\n",
            "Epoch 28/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0253 - acc: 0.9912 - val_loss: 1.7300 - val_acc: 0.7288\n",
            "Epoch 29/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0410 - acc: 0.9856 - val_loss: 1.9658 - val_acc: 0.6949\n",
            "Epoch 30/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0458 - acc: 0.9839 - val_loss: 1.8748 - val_acc: 0.7215\n",
            "Epoch 31/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0389 - acc: 0.9861 - val_loss: 2.3647 - val_acc: 0.6749\n",
            "Epoch 32/50\n",
            "50000/50000 [==============================] - 103s 2ms/step - loss: 0.0319 - acc: 0.9892 - val_loss: 1.9488 - val_acc: 0.7089\n",
            "Epoch 33/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0265 - acc: 0.9913 - val_loss: 1.9919 - val_acc: 0.7146\n",
            "Epoch 34/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0386 - acc: 0.9865 - val_loss: 2.2119 - val_acc: 0.7033\n",
            "Epoch 35/50\n",
            "50000/50000 [==============================] - 103s 2ms/step - loss: 0.0448 - acc: 0.9840 - val_loss: 2.0435 - val_acc: 0.7001\n",
            "Epoch 36/50\n",
            "50000/50000 [==============================] - 103s 2ms/step - loss: 0.0235 - acc: 0.9918 - val_loss: 1.9454 - val_acc: 0.7213\n",
            "Epoch 37/50\n",
            "50000/50000 [==============================] - 103s 2ms/step - loss: 0.0177 - acc: 0.9940 - val_loss: 1.9440 - val_acc: 0.7231\n",
            "Epoch 38/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0246 - acc: 0.9915 - val_loss: 2.3629 - val_acc: 0.6906\n",
            "Epoch 39/50\n",
            "50000/50000 [==============================] - 103s 2ms/step - loss: 0.0262 - acc: 0.9909 - val_loss: 2.2012 - val_acc: 0.7107\n",
            "Epoch 40/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0400 - acc: 0.9863 - val_loss: 2.1608 - val_acc: 0.6939\n",
            "Epoch 41/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0290 - acc: 0.9900 - val_loss: 2.0500 - val_acc: 0.7127\n",
            "Epoch 42/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0260 - acc: 0.9912 - val_loss: 2.0525 - val_acc: 0.7119\n",
            "Epoch 43/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0251 - acc: 0.9913 - val_loss: 2.0301 - val_acc: 0.7200\n",
            "Epoch 44/50\n",
            "50000/50000 [==============================] - 103s 2ms/step - loss: 0.0177 - acc: 0.9946 - val_loss: 2.1535 - val_acc: 0.7197\n",
            "Epoch 45/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0175 - acc: 0.9941 - val_loss: 2.3436 - val_acc: 0.7062\n",
            "Epoch 46/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0337 - acc: 0.9886 - val_loss: 2.3106 - val_acc: 0.7040\n",
            "Epoch 47/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0333 - acc: 0.9885 - val_loss: 2.1151 - val_acc: 0.7126\n",
            "Epoch 48/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0210 - acc: 0.9926 - val_loss: 2.2252 - val_acc: 0.7148\n",
            "Epoch 49/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0147 - acc: 0.9950 - val_loss: 2.0554 - val_acc: 0.7297\n",
            "Epoch 50/50\n",
            "50000/50000 [==============================] - 104s 2ms/step - loss: 0.0117 - acc: 0.9960 - val_loss: 2.2346 - val_acc: 0.7113\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f4a90221b00>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "metadata": {
        "id": "pTfZIGrf4Uyd",
        "colab_type": "code",
        "outputId": "a5c3d101-d49f-40df-e57d-8db0188efaa1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "score = model.evaluate(x_test, y_test, verbose=1)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "# Save the trained weights in to .h5 format\n",
        "model.save_weights(\"ASSIGNMENT_3C.h5\")\n",
        "print(\"Saved the model to disk\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 8s 783us/step\n",
            "Test loss: 2.2345538202285766\n",
            "Test accuracy: 0.7113\n",
            "Saved the model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Jiyb9TlVGsZs",
        "colab_type": "code",
        "outputId": "c979ca6c-f5a8-4d6f-a23e-7bdaef2332f0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "model.save_weights(\"ASSIGNMENT_3C.h5\")\n",
        "print(\"Saved the model to disk\")\n",
        "from google.colab import files\n",
        "\n",
        "files.download('ASSIGNMENT_3C.h5')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saved the model to disk\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "g9A3pesKbUJ8",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}